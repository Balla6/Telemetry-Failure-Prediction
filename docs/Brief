# Telemetry Failure Prediction — Brief & OKRs

## Problem (1-liner)
Predict service incidents (crashes or latency spikes) **15 minutes before they start** and raise **high-precision** alerts, with drift monitoring and a live dashboard.

## Product Context
A cloud Web API service with per-minute telemetry (`requests`, `errors`, `latency`, `CPU`, `memory`, `I/O`) across multiple `service_id`s.

## Incident Definition (what we predict)
An incident starts when **either** condition holds for **≥ 5 consecutive minutes**:
- `P95 latency > 1200 ms`, or
- `Error rate ≥ 5%` (i.e., `errors / requests`)

## Prediction Horizon (labeling)
At time `t`, the label `y(t) = 1` if an incident begins within `[t, t + 15 min]` for that `service_id`; otherwise `0`.

## Alert Policy (budget & hygiene)
- **Budget:** Max **5 alerts/day** (per environment)
- **Cooldown:** **30 minutes** per `service_id` after an alert
- **Aggregation:** Alert only if **2 consecutive predictions** exceed the threshold

## Success Criteria (ship bar)
- **Precision ≥ 0.80**
- **Recall ≥ 0.60**
- **Median lead time ≥ 10 min**
- **PR-AUC ≥ 0.65** on time-split test
- **Drift monitors** in place (data & concept) with documented actions

## Scope of Telemetry *(per minute unless noted)*
- `requests`, `errors`, `error_rate = errors/requests`
- `latency_p50`, `latency_p95`
- `cpu_pct`, `mem_pct`, `disk_io`, `net_in`, `net_out`
- `status_code_*` counts (e.g., `2xx/4xx/5xx`)
- `service_id`, `timestamp` (UTC)

## Non-Goals (for now)
- Root-cause localization per microservice
- Cost-aware auto-scaling control loops
- Real streaming infra (we’ll simulate micro-batches)

## Risks & Mitigations
- **Synthetic bias:** use multiple failure modes (traffic surges, resource saturation, dependency flakiness).
- **Label leakage:** strict time alignment & **rolling features only**.
- **Alert fatigue:** threshold tuned to alert budget **+ cooldown + aggregation**.

## OKRs (2-week target)

### O1: Catch incidents early with few false alarms
- **KR1:** Precision ≥ 0.80  
- **KR2:** Recall ≥ 0.60  
- **KR3:** Median lead time ≥ 10 min

### O2: Be trustworthy & maintainable
- **KR4:** Data contracts catch >95% schema/range breaks.
- **KR5:** Drift detection triggers on synthetic shifts with <5 min detection delay.

### O3: Make it demo-ready
- **KR6:** Dashboard shows KPIs, alert feed with SHAP reasons, drift panel, data quality panel.
- **KR7:** One-page exec memo and on-call runbook completed.

## Glossary
- **Lead time:** incident start time minus alert time.  
- **Drift:** change in data distribution (**data drift**) or model relationship (**concept drift**).  
- **Guardrails:** constraints like alert budget & cooldown.

